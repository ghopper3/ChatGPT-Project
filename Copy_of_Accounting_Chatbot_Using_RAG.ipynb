{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghopper3/ChatGPT-Project/blob/main/Copy_of_Accounting_Chatbot_Using_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drvKDDCYdPv2"
      },
      "source": [
        "# Building a Fine-Tuned RAG Chatbot with LangChain\n",
        "# (Part 2: Retrieval-Augmented Generation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In [Part 1](https://colab.research.google.com/drive/1nvAbE1bWQX_zHNCVEcyn2JSd-w1hYWc8?usp=sharing) of this project, we fine-tuned ChatGPT to be a finance and accounting chatbot. As a reminder, fine-tuning involves re-training a pre-trained LLM on a new dataset of labeled examples (in our case we used sample questions and ansswers from CPA and CFA exams). Fine-tuning is meant to improve the LLM's performance on specific tasks, such as text classification, language translation, or question answering.\n",
        "\n",
        "In this next step, we're going to use Retrieval Augemented Generation to focus our model on our desired goals even more.\n",
        "\n",
        "RAG involves augmenting a pre-trained LLM with a retrieval component. This allows the LLM to access and retrieve information from external knowledge bases, which can help to improve its accuracy and informativeness on knowledge-intensive tasks.\n",
        "\n",
        "So what is RAG?\n",
        "Retrieval-Augmented Generation (RAG) is a technique that improves the accuracy of large language models (LLMs) by allowing them to access and retrieve information from external knowledge bases. RAG models work by first retrieving a set of relevant documents from the knowledge base for the given query, and then generating the final output text by conditioning on the query and the retrieved documents. RAG models have the potential to revolutionize the way that we use LLMs for a variety of tasks, such as question answering, summarization, and translation.\n",
        "\n",
        "**Use fine-tuning:**\n",
        "*   When you have a labeled dataset of examples for the specific task that you want the LLM to perform.\n",
        "*   When you need the LLM to perform well on a specific task, even if it means sacrificing some performance on other tasks.\n",
        "\n",
        "\n",
        "**Use RAG:**\n",
        "*   When you need the LLM to be accurate and informative on a wide range of knowledge-intensive tasks.\n",
        "*   When you don't have a labeled dataset for the specific task that you want the LLM to perform.\n",
        "*   When you need the LLM to be able to access and retrieve information from external knowledge bases.\n",
        "\n",
        "In our case, we're using both in an effort to achieve even better results."
      ],
      "metadata": {
        "id": "iRNmeOd9ZhrA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrYjDJPhdPv3"
      },
      "source": [
        "This portion of the project is based on the work of James Briggs (https://aurelio.ai/) and his project: \"Building RAG Chatbots with LangChain.\"\n",
        "\n",
        "I am including James' instructions throughout, and adding my own as they relate specifically for our use case of building a Finance & Accounting Chatbot using a fine-tuned model with accounting skills.\n",
        "\n",
        "In this example, we'll work on building an AI chatbot from start-to-finish. We will be using LangChain, OpenAI, and Pinecone vector DB, to build a chatbot capable of learning from the external world using Retrieval Augmented Generation (RAG).\n",
        "\n",
        "Our dataset will include accounting texts, including \"Internal Control Strategies for Compliance with the Sarbanes-Oxley Act of 2002\" and the \"Auditing Standards of the Public Company Accounting Oversight Board\" to help our chatbot answer questions about accounting specific topics.\n",
        "\n",
        "By the end of the example we'll have a functioning chatbot and RAG pipeline that can hold a conversation and provide informative responses based on a knowledge base.\n",
        "\n",
        "### Before you begin\n",
        "\n",
        "You'll need to get an [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCqAdjmjdPv3"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YkGXuEddPv3"
      },
      "source": [
        "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
        "\n",
        "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
        "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
        "- **datasets**: This library provides a vast array of datasets for machine learning. We'll use it to load our knowledge base for the chatbot.\n",
        "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
        "\n",
        "You can install these libraries using pip like this:\n",
        "\n",
        "(Note: You may have to run this twice if you get an error message about dependencies.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab8yhFfRdPv4"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.0.292 \\\n",
        "    openai==0.28.0 \\\n",
        "    datasets==2.10.1 \\\n",
        "    pinecone-client==2.2.4 \\\n",
        "    tiktoken==0.5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure we have the latest version of pandas"
      ],
      "metadata": {
        "id": "mFJJxm-jTElq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas\n"
      ],
      "metadata": {
        "id": "9sDyRTU1S9Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFVUQWRPdPv6"
      },
      "source": [
        "### Building a Chatbot (no RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rf3TjOOdPv6"
      },
      "source": [
        "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "Note for the model here, we are using our fine-tuned model that we used in Part 1 of this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04NJe7PVdPv6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='LINK_TO_YOUR_FINE_TUNED_MODEL_FROM_PART_1' # Alternatively, you can use GPT 3.5 or GPT 4 OTS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5P1R_s-dPv7"
      },
      "source": [
        "Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n",
        "\n",
        "```\n",
        "System: You are a helpful accounting assistant.\n",
        "\n",
        "User: Hi AI, how are you today?\n",
        "\n",
        "Assistant: I'm great thank you. How can I help you?\n",
        "\n",
        "User: I'd like to understand FASB Update ASU 2016-13.\n",
        "\n",
        "Assistant:\n",
        "```\n",
        "\n",
        "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
        "\n",
        "```python\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful accounting assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
        "    {\"role\": \"user\", \"content\": \"I'd like to understand FASB Update ASU 2016-13.\"}\n",
        "]\n",
        "```\n",
        "\n",
        "In LangChain there is a slightly different format. We use three _message_ objects like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBlgvt5IdPv8"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful accounting assistant.\"),\n",
        "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
        "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
        "    HumanMessage(content=\"I'd like to understand FASB Update ASU 2016-13.\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjlTYwhIdPv8"
      },
      "source": [
        "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
        "\n",
        "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb2djmckdPv9"
      },
      "outputs": [],
      "source": [
        "res = chat(messages)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY11MUeIdPv-"
      },
      "source": [
        "In response we get another AI message object. We can print it more clearly like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPTeuLftdPv-"
      },
      "outputs": [],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjco7hvVdPv_"
      },
      "source": [
        "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kV6z6yfdPv_"
      },
      "outputs": [],
      "source": [
        "# add latest AI response to messages\n",
        "messages.append(res)\n",
        "\n",
        "# now create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=\"How might this impact my business?\"\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to chat-gpt\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL_cqus7dPv_"
      },
      "source": [
        "### Dealing with Hallucinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3PDwSnmdPv_"
      },
      "source": [
        "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
        "\n",
        "By default, LLMs have no access to the external world.\n",
        "\n",
        "The result of this is very clear when we ask LLMs about more recent information, like ASU 2023-02, which was issued in March of 2023, which addresses the accounting for investments in certain tax credit structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7snaPi3UdPwA"
      },
      "outputs": [],
      "source": [
        "# add latest AI response to messages\n",
        "messages.append(res)\n",
        "\n",
        "# now create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=\"What can you tell me about ASU 2023-05?\"\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to OpenAI\n",
        "res = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyDAd8SIdPwB"
      },
      "outputs": [],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MH0RustdPwB"
      },
      "source": [
        "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear from this answer that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect.\n",
        "\n",
        "OpenAI have since adjusted the behavior for this particular example as we can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IPe53DedPwB"
      },
      "outputs": [],
      "source": [
        "# add latest AI response to messages\n",
        "messages.append(res)\n",
        "\n",
        "# now create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=\"Can you tell me what ASU 2023-05 says about the initial measurement of a joint venture’s total net assets?\"\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to OpenAI\n",
        "res = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfoxEABCdPwC"
      },
      "outputs": [],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYwp0MnbdPwC"
      },
      "source": [
        "There is another way of feeding knowledge into LLMs. It is called _source knowledge_ and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLrJq7IhdPwC"
      },
      "outputs": [],
      "source": [
        "llmchain_information = [\n",
        "    \"ASU 2023-05 defines a joint venture as A joint venture is the formation of a new entity without an accounting acquirer. The formation of a joint venture is the creation of a new reporting entity, and none of the assets and/or businesses contributed to the joint venture are viewed as having survived the combination as an independent entity—that is, an accounting acquirer will not be identified.\",\n",
        "    \"Under ASU 2023-05, A joint venture measures its identifiable net assets and goodwill, if any, at the formation date. The joint venture formation date is the date on which an entity initially meets the definition of a joint venture.\",\n",
        "    \"Initial measurement of a joint venture’s total net assets is equal to the fair value of 100 percent of the joint venture’s equity. The amendments require that a joint venture measure its total net assets upon formation as the fair value of the joint venture as a whole. The fair value of the joint venture as a whole equals the fair value of 100 percent of a joint venture’s equity immediately following formation (including any noncontrolling interest in the net assets recognized by the joint venture).\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(llmchain_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUG0kRiMdPwD"
      },
      "source": [
        "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiZnbnhddPwD"
      },
      "outputs": [],
      "source": [
        "query = \"Can you tell me what ASU 2023-05 says about the initial measurement of a joint venture’s total net assets?\"\n",
        "\n",
        "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "Contexts:\n",
        "{source_knowledge}\n",
        "\n",
        "Query: {query}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEvwRPEkdPwD"
      },
      "source": [
        "Now we feed this into our chatbot as we were before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9wbxkQsdPwD"
      },
      "outputs": [],
      "source": [
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augmented_prompt\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "# send to OpenAI\n",
        "res = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3vgY6x-dPwD"
      },
      "outputs": [],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTK5x7KfdPwE"
      },
      "source": [
        "The quality of this answer is much better. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem — how do we get this information in the first place?\n",
        "\n",
        "This is where we will use Pinecone and vector databases, but first, we'll need a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q89mSn_edPwE"
      },
      "source": [
        "### Importing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIGwThpNdPwE"
      },
      "source": [
        "In this task, we will be importing our data. We will be using a cuople of accounting specific PDFs for this test. Specifically, we are using \"Internal Control Strategies for Compliance with the Sarbanes-Oxley Act of 2002\" and the \"Auditing Standards of the Public Company Accounting Oversight Board.\"\n",
        "However, once the model works, we could use a long list of accounting references with documents like:\n",
        "\n",
        "**Regulatory and Standards Manuals**\n",
        "1.\tFASB Accounting Standards Codification (ASC)\n",
        "2.\tInternational Financial Reporting Standards (IFRS) Handbook\n",
        "3.\tPCAOB Auditing Standards Manual\n",
        "4.\tSarbanes-Oxley Act (SOX) Compliance Guide\n",
        "\n",
        "Before we can import our PDFs, we have to install some additional software and mount our Google Drive to access the documents. We are also going to try to do some data cleanup by converting the files to text documents.\n",
        "\n",
        "The code to do this is below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install PyPDF2\n",
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "9StNgvzEH6ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWh0AghIdPwF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from pinecone import Index\n",
        "import requests\n",
        "\n",
        "# Define the API key\n",
        "API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "# Create a Pinecone index\n",
        "index = Index(\"my-index\")\n",
        "\n",
        "# Define a function to extract text from a PDF file\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Define a function to split text into chunks\n",
        "def split_text_into_chunks(text, chunk_size=1000):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunks.append(text[i : i + chunk_size])\n",
        "    return chunks\n",
        "\n",
        "# Function to embed documents using BERT\n",
        "def embed_documents(texts, model):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "    return embeddings\n",
        "\n",
        "# Initialize tokenizer and model for BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# If you're using google drive to store your documents, you need to mount it and insert the path to where your files are stored\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your PDF files in Google Drive\n",
        "pdf_dir = \"INSERT_PATH_TO_YOUR_DIRECTORY\"\n",
        "\n",
        "# Create a list of dictionaries containing the text and filename for each PDF file\n",
        "pdf_data = []\n",
        "for filename in os.listdir(pdf_dir):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(pdf_dir, filename)\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "        chunks = split_text_into_chunks(text)\n",
        "        for chunk in chunks:\n",
        "            pdf_data.append({\"text\": chunk, \"filename\": filename})\n",
        "\n",
        "# Convert the list of dictionaries to a dictionary with column names as keys and lists of values as values\n",
        "pdf_data_dict = {}\n",
        "for key in pdf_data[0].keys():\n",
        "    pdf_data_dict[key] = [d[key] for d in pdf_data]\n",
        "\n",
        "# Convert the dictionary to a dataset object\n",
        "dataset = Dataset.from_dict(pdf_data_dict)\n",
        "\n",
        "# Add the doi column to the dataset\n",
        "dataset = dataset.map(lambda x: {\"doi\": x[\"filename\"].split(\".\")[0], **x})\n",
        "\n",
        "# Convert the dataset to a pandas DataFrame object\n",
        "data = dataset.to_pandas()\n",
        "\n",
        "# Remove the filename column from the DataFrame\n",
        "data = data.drop(columns=[\"filename\"])\n",
        "\n",
        "# Set the batch size and loop over the data with a progress bar\n",
        "batch_size = 10\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i + batch_size)\n",
        "    batch = data.iloc[i : i_end]\n",
        "    ids = [f\"{x['doi']}-{i}-{j}\" for i, x in batch.iterrows() for j in range(len(batch))]\n",
        "    texts = [x[\"text\"] for _, x in batch.iterrows()]\n",
        "\n",
        "# Embed text\n",
        "embeds = embed_documents(texts, model)\n",
        "\n",
        "# Split metadata into smaller batches and store each batch separately\n",
        "metadata = [{\"text\": x[\"text\"]} for _, x in batch.iterrows()]\n",
        "metadata_batches = [metadata[i : i + 10] for i in range(0, len(metadata), 10)]\n",
        "\n",
        "# Loop through metadata batches\n",
        "for j, metadata_batch in enumerate(metadata_batches):\n",
        "    # Generate unique IDs for each metadata batch\n",
        "    metadata_ids = [f\"{x['doi']}-{i}-{j}\" for i, x in batch.iterrows()]\n",
        "\n",
        "    # Add to Pinecone\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGOLQqmJdPwF"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZeXPN1bdPwF"
      },
      "source": [
        "#### Dataset Overview\n",
        "\n",
        "The dataset we are using comes from the two PDFs we loaded, but with a larger database and more documentation we could increase our model's knowledge base. The PDFs were each several hundred pages, so we had to break them into smaller \"chunks\" so that our model could read them. Each entry in the dataset represents a \"chunk\" of text from these PDFs.\n",
        "\n",
        "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, they cannot answer questions about information that wasn't available until after they were trained. This method allows us to add new information as it's available and to focus the model for our specific needs -- in this case, building a finance and accounting expert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFzKlAwrdPwG"
      },
      "source": [
        "### Task 4: Building the Knowledge Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOvLCVardPwG"
      },
      "source": [
        "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
        "\n",
        "We begin by initializing our connection to Pinecone, this requires a [free API key](https://app.pinecone.io)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXiTOTXXdPwP"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "# get API key from app.pinecone.io and environment from console\n",
        "pinecone.init(\n",
        "    api_key=os.environ.get('PINECONE_API_KEY') or 'YOUR_PINECONE_API_KEY',\n",
        "    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohyeF0y5dPwQ"
      },
      "source": [
        "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQsOWbKcdPwQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "index_name = 'acctg-chatbot'\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,\n",
        "        metric='cosine'\n",
        "    )\n",
        "    # wait for index to finish initialization\n",
        "    while not pinecone.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pinecone.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1-FAxPodPwQ"
      },
      "source": [
        "Then we connect to the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1reo4jMdPwR"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iq8zdACdPwR"
      },
      "source": [
        "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model — we can access it via LangChain like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd9iYAsZdPwR"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlH8fcKpdPwS"
      },
      "source": [
        "Using this model we can create embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrIpaKofdPwS"
      },
      "outputs": [],
      "source": [
        "# Define a list of dictionaries containing the text and metadata for each chunk\n",
        "chunks = [\n",
        "    {'text': 'This is the text of the first chunk.', 'source': 'source1', 'title': 'title1', 'doi': 'doi1', 'chunk-id': 'chunk1'},\n",
        "    {'text': 'This is the text of the second chunk.', 'source': 'source2', 'title': 'title2', 'doi': 'doi2', 'chunk-id': 'chunk2'},\n",
        "    {'text': 'This is the text of the third chunk.', 'source': 'source3', 'title': 'title3', 'doi': 'doi3', 'chunk-id': 'chunk3'},\n",
        "    # Add more chunks here\n",
        "]\n",
        "\n",
        "# Split the chunks into batches of 100\n",
        "batch_size = 100\n",
        "chunks_batches = [chunks[i:i+batch_size] for i in range(0, len(chunks), batch_size)]\n",
        "metadata_batches = []\n",
        "\n",
        "# Loop over the chunks in batches\n",
        "for chunks_batch in tqdm(chunks_batches):\n",
        "    # Get the metadata for each chunk in the batch\n",
        "    metadata_batch = []\n",
        "    for chunk in chunks_batch:\n",
        "        metadata = {\n",
        "            'text': chunk['text'],\n",
        "            'source': chunk['source'],\n",
        "            'title': chunk['title']\n",
        "        }\n",
        "        metadata_batch.append(metadata)\n",
        "    # Store the metadata for the batch in a separate vector\n",
        "    metadata_batches.append(metadata_batch)\n",
        "\n",
        "# Embed the chunks and store the embeddings and metadata in Pinecone\n",
        "for i, chunks_batch in enumerate(tqdm(chunks_batches)):\n",
        "    # Get the embeddings for the chunks in the batch\n",
        "    embeddings_batch = embed_model.embed_documents([chunk['text'] for chunk in chunks_batch])\n",
        "    # Get the metadata for the batch\n",
        "    metadata_batch = metadata_batches[i]\n",
        "    # Generate unique ids for each chunk in the batch\n",
        "    ids_batch = [f\"{chunk['doi']}-{chunk['chunk-id']}\" for chunk in chunks_batch]\n",
        "    # Split the metadata into smaller batches\n",
        "    metadata_batches_split = [metadata_batch[j:j+batch_size] for j in range(0, len(metadata_batch), batch_size)]\n",
        "    # Store the embeddings and metadata for the batch in Pinecone\n",
        "    for j, metadata_batch_split in enumerate(metadata_batches_split):\n",
        "        ids_batch_split = ids_batch[j*batch_size:(j+1)*batch_size]\n",
        "        embeddings_batch_split = embeddings_batch[j*batch_size:(j+1)*batch_size]\n",
        "        index.upsert(vectors=zip(ids_batch_split, embeddings_batch_split, metadata_batch_split))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MtEArSpdPwS"
      },
      "source": [
        "From this we get two (aligning to our 1,970 chunks of text) 1536-dimensional embeddings.\n",
        "\n",
        "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTi0JmGKdPwT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "# Define a function to extract text from a PDF file\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with open(file_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = ''\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Define the path to your PDF files\n",
        "pdf_dir = 'INSERT_PATH_TO_YOUR_FILES'\n",
        "\n",
        "# Create a list of dictionaries containing the text and filename for each PDF file\n",
        "pdf_data = []\n",
        "for filename in os.listdir(pdf_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        file_path = os.path.join(pdf_dir, filename)\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "        pdf_data.append({'text': text, 'filename': filename})\n",
        "\n",
        "# Convert the list of dictionaries to a dictionary with column names as keys and lists of values as values\n",
        "pdf_data_dict = {}\n",
        "for key in pdf_data[0].keys():\n",
        "    pdf_data_dict[key] = [d[key] for d in pdf_data]\n",
        "\n",
        "# Convert the dictionary to a dataset object\n",
        "dataset = Dataset.from_dict(pdf_data_dict)\n",
        "\n",
        "# Add the doi column to the dataset\n",
        "dataset = dataset.map(lambda x: {'doi': x['filename'].split('.')[0], **x})\n",
        "\n",
        "# Convert the dataset to a pandas DataFrame object\n",
        "data = dataset.to_pandas()\n",
        "\n",
        "# Remove the filename column from the DataFrame\n",
        "data = data.drop(columns=['filename'])\n",
        "\n",
        "# Set the batch size and loop over the data with a progress bar\n",
        "batch_size = 100\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    # get batch of data\n",
        "    batch = data.iloc[i:i_end]\n",
        "    # generate unique ids for each chunk\n",
        "    ids = [f\"{x['doi']}-{i}\" for i, x in batch.iterrows()]\n",
        "    # get text to embed\n",
        "    texts = [x['text'][:1000] for _, x in batch.iterrows()]  # store only the first 1000 characters of the text\n",
        "    # embed text\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    # get metadata to store in Pinecone\n",
        "    metadata = [\n",
        "        {'text': x['text'][:1000]} for _, x in batch.iterrows()  # store only the first 1000 characters of the text\n",
        "    ]\n",
        "    # split metadata into smaller batches and store each batch separately\n",
        "    metadata_batches = [metadata[i:i+10] for i in range(0, len(metadata), 10)]\n",
        "    for metadata_batch in metadata_batches:\n",
        "        # add to Pinecone\n",
        "        index.upsert(vectors=zip(ids, embeds, metadata_batch))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMqCTJ9odPwT"
      },
      "source": [
        "We can check that the vector index has been populated using `describe_index_stats` like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jptyhCPtdPwT"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPCC2fa5dPwU"
      },
      "source": [
        "#### Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UicCfUcdPwU"
      },
      "source": [
        "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot. To do that we'll be asking specific questions from the documents we uploaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNVhIvMfdPwU"
      },
      "source": [
        "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a5aQdtidPwV"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"  # the metadata field that contains our text\n",
        "\n",
        "# initialize the vector store object\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBohSiOdPwV"
      },
      "source": [
        "Using this `vectorstore` we can already query the index and see if we have any relevant information based on text from one of the uploaded PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asLOS0ymdPwV"
      },
      "outputs": [],
      "source": [
        "query = \"What are two different types of fraud?\"\n",
        "\n",
        "vectorstore.similarity_search(query, k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxdiwl3UdPwW"
      },
      "source": [
        "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ho06VmPdPwW"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str):\n",
        "    # get top 3 results from knowledge base\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    # get the text from the results\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    # feed into an augmented prompt\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOXO50PVdPwW"
      },
      "source": [
        "Using this we produce an augmented prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9ihRuUHdPwW"
      },
      "outputs": [],
      "source": [
        "print(augment_prompt(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syv5QH2-dPwX"
      },
      "source": [
        "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqnbggqRdPwX"
      },
      "outputs": [],
      "source": [
        "# create a new user prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(query)\n",
        ")\n",
        "# add to messages\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try it out for yourself\n",
        "Enter your finance and accounting question here as you would in your favorite LLM-driven Chatbot.\n"
      ],
      "metadata": {
        "id": "SWebaabEF2XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Initialize the list to store messages\n",
        "messages = []\n",
        "\n",
        "# Function to augment the prompt\n",
        "def augment_prompt(query: str):\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt\n",
        "\n",
        "# Function to handle button click\n",
        "def on_button_click(b):\n",
        "    clear_output()\n",
        "    display(textbox, button)\n",
        "\n",
        "    query = textbox.value  # User input\n",
        "    augmented_query = augment_prompt(query)  # Augment the prompt\n",
        "\n",
        "    prompt = HumanMessage(content=augmented_query)  # Create new user prompt\n",
        "    messages.append(prompt)  # Add to existing messages\n",
        "\n",
        "    res = chat(messages)  # Get the response\n",
        "\n",
        "    print(res.content)  # Display the response\n",
        "\n",
        "# Create a text box for user input\n",
        "textbox = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter your prompt here',\n",
        "    description='Prompt:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Create a button to submit the prompt\n",
        "button = widgets.Button(description=\"Submit\")\n",
        "\n",
        "# Display textbox and button\n",
        "display(textbox, button)\n",
        "\n",
        "# Attach the button click event to the function\n",
        "button.on_click(on_button_click)\n"
      ],
      "metadata": {
        "id": "Rl-Zfm7tGCsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "redacre",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}